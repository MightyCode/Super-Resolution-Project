{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objective is to test some images and see the metric results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from src.dataset.PatchImageTool import PatchImageTool\n",
    "from src.utils.PytorchUtil import PytorchUtil as torchUtil\n",
    "from src.utils.PlotUtils import PlotUtils\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.metrics as metrics\n",
    "import platform  # Import the platform module to detect the OS\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import platform\n",
    "import os\n",
    "\n",
    "device = None\n",
    "if platform.system() == 'Windows':  # Check if the OS is Windows\n",
    "    import torch_directml  # Import torch_directml only on Windows\n",
    "    device = torch_directml.device()\n",
    "\n",
    "force_cpu = True\n",
    "\n",
    "if not device:\n",
    "    if torch.cuda.is_available() and not force_cpu:\n",
    "        device = torch.device('cuda')\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.InitModel import InitModel\n",
    "\n",
    "UPSCALE_FACTOR = 2\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "PATCH_RESIZE_SIZE = PATCH_SIZE // UPSCALE_FACTOR\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 256\n",
    "USE_PREDICTION_BY_PATCH = False \n",
    "\n",
    "IMAGE_DIM = \"1920x1080\"\n",
    "\n",
    "CHANNELS = [\"b\", \"g\", \"r\"]\n",
    "CHANNELS_POSITION = {\"b\": 0, \"g\": 1, \"r\": 2, \"d\": 3, \"s\": 4}\n",
    "\n",
    "SEED = None\n",
    "\n",
    "IMAGE = \"example.png\"\n",
    "\n",
    "PATH = \"results/weights-upscale-residual-lpips-v.2\"\n",
    "NAME = PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = InitModel.create_model_static(NAME, PATH, {\"learningRate\": LEARNING_RATE, \"channels\" : CHANNELS}, UPSCALE_FACTOR, device)\n",
    "\n",
    "if SEED:\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = \"resources\"\n",
    "\n",
    "hr_data_path = os.path.join(resources_path, IMAGE)\n",
    "\n",
    "hr_data_np = torchUtil.open_data(hr_data_path)\n",
    "#hr_data_np = torchUtil.norm_numpy_image(hr_data_np)\n",
    "\n",
    "hr_img_size = (hr_data_np.shape[1], hr_data_np.shape[0])\n",
    "\n",
    "# apply transform to image\n",
    "hr_data_tensor = image_transform(hr_data_np).to(device)\n",
    "\n",
    "hr_img_tensor = torchUtil.filter_data(hr_data_tensor, {\"b\", \"g\", \"r\"}, CHANNELS_POSITION)\n",
    "hr_img_np = torchUtil.tensor_to_numpy(hr_img_tensor)\n",
    "\n",
    "# divide image image by x\n",
    "resized_data_np = torchUtil.resize_data(hr_data_np, (hr_img_size[0] // UPSCALE_FACTOR, hr_img_size[1] // UPSCALE_FACTOR), CHANNELS, CHANNELS_POSITION)\n",
    "resized_data_tensor = image_transform(resized_data_np).to(device)\n",
    "resized_img_tensor = torchUtil.filter_data(resized_data_tensor, {\"b\", \"g\", \"r\"}, CHANNELS_POSITION)\n",
    "\n",
    "resized_img_np = torchUtil.tensor_to_numpy(resized_img_tensor)\n",
    "resized_img_size = (resized_img_np.shape[0], resized_img_np.shape[1])\n",
    "\n",
    "print(resized_img_tensor.shape, hr_img_tensor.shape)\n",
    "\n",
    "PlotUtils.show_high_low_res_images([resized_img_tensor], [hr_img_tensor], upscale_factor_list=[UPSCALE_FACTOR], plot_title=\"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patch_width = math.ceil(resized_img_size[0] / PATCH_RESIZE_SIZE)\n",
    "num_patch_height = math.ceil(resized_img_size[1] / PATCH_RESIZE_SIZE)\n",
    "\n",
    "num_patch_total = num_patch_width * num_patch_height\n",
    "\n",
    "resized_data_patch_tensors = PatchImageTool.get_patchs_from_image(resized_data_tensor, patch_size=PATCH_RESIZE_SIZE)\n",
    "\n",
    "resized_img_patch_tensors = []\n",
    "for patch in resized_data_patch_tensors:\n",
    "    resized_img_patch_tensors.append(torchUtil.filter_data(patch, {\"b\", \"g\", \"r\"}, CHANNELS_POSITION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = min(5, num_patch_width)\n",
    "num_rows = min(5, num_patch_height)\n",
    "\n",
    "# Show some image patches\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(3 * num_cols, 3 * num_rows))\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # ndarray\n",
    "        \n",
    "        ax[i, j].imshow(torchUtil.tensor_to_image(resized_img_patch_tensors[i * num_patch_width + j]))\n",
    "        ax[i, j].set_title(f\"Patch {i * num_patch_width + j}\")\n",
    "        ax[i, j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model predict one patch\n",
    "patch_index = 0\n",
    "\n",
    "resized_data_patch_tensor = resized_data_patch_tensors[patch_index].to(device)\n",
    "resized_img_patch_tensor = resized_img_patch_tensors[patch_index]\n",
    "print(\"Patch tensor size\", resized_data_patch_tensor.shape, \"type\", resized_data_patch_tensor.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_img_tensors = exp.net(resized_data_patch_tensor)\n",
    "    pred_img_tensor = pred_img_tensors.squeeze(0)\n",
    "    \"\"\"prediction = exp.net(prediction)\n",
    "    prediction = prediction.squeeze(0)\"\"\"\n",
    "\n",
    "    pred_img_np = torchUtil.tensor_to_numpy(pred_img_tensor)\n",
    "\n",
    "    print(\"Prediction tensor size\", pred_img_np.shape, \"type\", pred_img_np.dtype)\n",
    "\n",
    "    bicubic_img_np = torchUtil.resize_tensor_to_numpy(resized_img_patch_tensor, (PATCH_SIZE, PATCH_SIZE))\n",
    "\n",
    "    subtraction_img_np = torchUtil.norm_numpy_image(pred_img_np - bicubic_img_np)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 10))\n",
    "ax[0].imshow(torchUtil.tensor_to_image(resized_img_patch_tensor))\n",
    "ax[0].set_title(f\"Low res patch\")\n",
    "\n",
    "ax[1].imshow(torchUtil.numpy_to_image(pred_img_np))\n",
    "ax[1].set_title(f\"Prediction\")\n",
    "\n",
    "ax[2].imshow(torchUtil.numpy_to_image(subtraction_img_np))\n",
    "ax[2].set_title(f\"Subtraction\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use image\n",
    "#image_upscaled_torch = prediction\n",
    "# Use patches reconstructed image\n",
    "pred_img_torch = None\n",
    "\n",
    "if USE_PREDICTION_BY_PATCH:\n",
    "    pred_img_torch = PatchImageTool.predict_image_from_image_patches(\n",
    "                        exp, hr_img_size, resized_data_patch_tensors, \n",
    "                        device, \n",
    "                        patch_size=PATCH_RESIZE_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "    print(\"Used prediction by patch\")\n",
    "else :\n",
    "    pred_img_torch = exp.net(resized_data_tensor.unsqueeze(0)).squeeze(0)    \n",
    "    print(\"Used prediction by image\")\n",
    "\n",
    "image_to_show = torchUtil.numpy_to_image(hr_img_np)\n",
    "\n",
    "pred_img_np = torchUtil.tensor_to_numpy(pred_img_torch)\n",
    "pred_img = torchUtil.numpy_to_image(pred_img_np)\n",
    "\n",
    "bicubic_img_np = torchUtil.resize_tensor_to_numpy(resized_img_tensor, (hr_img_size[1], hr_img_size[0]))\n",
    "bicubic_img = torchUtil.numpy_to_image(bicubic_img_np)\n",
    "\n",
    "substract_img_np = torchUtil.norm_numpy_image(bicubic_img_np - pred_img_np)\n",
    "substract_img = torchUtil.numpy_to_image(substract_img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the original image, the predicted and the substracted image\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "ax[0].imshow(bicubic_img)\n",
    "ax[0].set_title(f\"Bicubic image {bicubic_img.shape}\")\n",
    "\n",
    "ax[1].imshow(pred_img)\n",
    "ax[1].set_title(f\"Upscaled image {pred_img.shape}\")\n",
    "\n",
    "ax[2].imshow(substract_img, vmin=substract_img_np.min(), vmax=substract_img_np.max())\n",
    "ax[2].set_title(f\"Substracted image {substract_img.shape}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PSNR and SSIM\n",
    "psnr = metrics.peak_signal_noise_ratio(hr_img_np, pred_img_np, data_range=1)\n",
    "ssim = metrics.structural_similarity(hr_img_np, pred_img_np, \\\n",
    "                                     win_size=7, data_range=1, multichannel=True, channel_axis=2)\n",
    "\n",
    "print(f\"Model PSNR: {psnr} SSIM: {ssim}\")\n",
    "\n",
    "bicubic_psnr = metrics.peak_signal_noise_ratio(hr_img_np, bicubic_img_np, data_range=1)\n",
    "bicubic_ssim = metrics.structural_similarity(hr_img_np, bicubic_img_np, \\\n",
    "                                     win_size=7, data_range=1, multichannel=True, channel_axis=2)\n",
    "\n",
    "print(f\"Bicubic PSNR: {bicubic_psnr} SSIM: {bicubic_ssim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the image results\n",
    "\n",
    "output_path = \"results/examples/\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# empty the output folder\n",
    "\n",
    "for filename in os.listdir(output_path):\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Save the original image\n",
    "cv2.imwrite(os.path.join(output_path, \"original.png\"), (hr_img_np * 255.0).astype(np.uint8))\n",
    "# Save the upscaled image\n",
    "cv2.imwrite(os.path.join(output_path, \"upscaled.png\"), (pred_img_np * 255.0).astype(np.uint8))\n",
    "# Save the bilinear image\n",
    "cv2.imwrite(os.path.join(output_path, \"bicubic.png\"), (bicubic_img_np * 255.0).astype(np.uint8))\n",
    "# Save the substracted image\n",
    "cv2.imwrite(os.path.join(output_path, \"substracted.png\"), (torchUtil.numpy_to_image(substract_img_np) * 255.0).astype(np.uint8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
